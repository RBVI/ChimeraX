{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alphafold21_predict_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GbA23Exts88C"
      },
      "outputs": [],
      "source": [
        "# ================================================================================================\n",
        "# Google Colab code for running an AlphaFold structure prediction.\n",
        "#\n",
        "\n",
        "# Make sure Google Colab virtual machine has a GPU\n",
        "def check_for_gpu():\n",
        "    import os\n",
        "    have_gpu = (int(os.environ.get('COLAB_GPU',1)) > 0)\n",
        "    if have_gpu:\n",
        "        print ('Have Colab GPU runtime')\n",
        "    else:\n",
        "        raise RuntimeError('Require Colab GPU runtime.\\n' +\n",
        "                           'Change GPU with Colab menu\\n' +\n",
        "                           'Runtime -> Change Runtime Type -> Hardware accelerator -> GPU.')\n",
        "\n",
        "def is_alphafold_installed():\n",
        "    try:\n",
        "        import alphafold\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def install_alphafold(\n",
        "        alphafold_git_repo = 'https://github.com/deepmind/alphafold',\n",
        "        alphafold_version = 'v2.2.0',\n",
        "        alphafold_parameters = ' https://storage.googleapis.com/alphafold/alphafold_params_2022-03-02.tar',\n",
        "        bond_parameters = 'https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt',\n",
        "        install_log = 'install_log.txt'):\n",
        "\n",
        "    params_dir = './alphafold/data/params'\n",
        "    import os.path\n",
        "    params_path = os.path.join(params_dir, os.path.basename(alphafold_parameters))\n",
        "\n",
        "    cmds = f'''\n",
        "# Exit if any command fails\n",
        "set -e\n",
        "\n",
        "# Uninstall Google Colab default tensorflow\n",
        "pip3 uninstall -y tensorflow\n",
        "\n",
        "# Get AlphaFold from GitHub and install it\n",
        "git clone --branch {alphafold_version} {alphafold_git_repo} alphafold\n",
        "# Install versions of dependencies specified in requirements.txt\n",
        "# Alphafold fails because jax==0.2.14 is incompatible with much newer jaxlib=0.1.70\n",
        "# resulting in error no module jax.experimental.compilation_cache.  The chex\n",
        "# package brings in jax 0.2.19 and jaxlib 0.1.70 but then jax is uninstalled\n",
        "# and replaced with 0.2.14 but jaxlib is not reverted to an older version.\n",
        "# Also need to get jaxlib from google rather than pypi to have cuda support.\n",
        "pip3 install -r ./alphafold/requirements.txt\n",
        "# Update jax\n",
        "pip3 install --upgrade jaxlib==0.3.2+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "pip3 install jax==0.3.4\n",
        "pip3 install --no-dependencies ./alphafold\n",
        "#pip3 install ./alphafold\n",
        "\n",
        "# Get AlphaFold parameters, 3.5 Gbytes,\n",
        "# Ten models model_1, model_2, ..., model_5, model_1_ptm, ..., model_5_ptm.\n",
        "mkdir -p \"{params_dir}\"\n",
        "wget -q -O \"{params_path}\" {alphafold_parameters}\n",
        "tar --extract --verbose --file=\"{params_path}\" --directory=\"{params_dir}\" --preserve-permissions\n",
        "rm \"{params_path}\"\n",
        "\n",
        "# Get standard bond length and bond angle parameters\n",
        "mkdir -p /content/alphafold/alphafold/common\n",
        "wget -q -P /content/alphafold/alphafold/common {bond_parameters}\n",
        "cp -f /content/alphafold/alphafold/common/stereo_chemical_props.txt /usr/local/lib/python3.7/dist-packages/alphafold/common/\n",
        "\n",
        "# Create a ramdisk to store a database chunk to make jackhmmer run fast.\n",
        "# Module alphafold.data.tools.jackhmmer makes use of this /tmp/ramdisk.\n",
        "sudo mkdir -m 777 --parents /tmp/ramdisk\n",
        "sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n",
        "'''\n",
        "    run_shell_commands(cmds, 'install_alphafold.sh', install_log)\n",
        "    \n",
        "def run_shell_commands(commands, filename, install_log):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(commands)\n",
        "\n",
        "    # The -x option logs each command with a prompt in front of it.\n",
        "    !bash -x \"{filename}\" >> \"{install_log}\" 2>&1\n",
        "    if _exit_code != 0:\n",
        "        raise RuntimeError('Error running shell script %s, output in log file %s'\n",
        "                           % (filename, install_log))\n",
        "    \n",
        "def install_hmmer(install_log = 'install_log.txt'):\n",
        "    # Install HMMER package in /usr/bin\n",
        "    cmds = '''sudo apt install --quiet --yes hmmer'''\n",
        "    run_shell_commands(cmds, 'install_hmmer.sh', install_log)\n",
        "\n",
        "def install_matplotlib(install_log = 'install_log.txt'):\n",
        "    # Install matplotlib for plotting alignment coverage\n",
        "    cmds = '''pip install matplotlib'''\n",
        "    run_shell_commands(cmds, 'install_matplotlib.sh', install_log)\n",
        "\n",
        "def is_openmm_installed():\n",
        "    try:\n",
        "        import simtk.openmm\n",
        "    except:\n",
        "        return False\n",
        "    return True\n",
        "    \n",
        "def install_openmm(\n",
        "        conda_install_sh = 'https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh',\n",
        "        install_log = 'install_log.txt'):\n",
        "    '''Must install alphafold first since an openmm patch from alphafold is used.'''\n",
        "    # Install Conda\n",
        "    import os.path\n",
        "    conda_install = os.path.join('/tmp', os.path.basename(conda_install_sh))\n",
        "    cmds = f'''\n",
        "# Exit if any command fails\n",
        "set -e\n",
        "\n",
        "wget -q -P /tmp {conda_install_sh} \\\n",
        "    && bash \"{conda_install}\" -b -p /opt/conda -f \\\n",
        "    && rm \"{conda_install}\"\n",
        "\n",
        "# Install Python, OpenMM and pdbfixer in Conda\n",
        "/opt/conda/bin/conda update -qy conda && \\\n",
        "    /opt/conda/bin/conda install -qy -c conda-forge python=3.7 openmm=7.5.1 cudatoolkit=11.2.2 pdbfixer\n",
        "\n",
        "# Patch OpenMM\n",
        "(cd /opt/conda/lib/python3.7/site-packages/ && \\\n",
        "    patch -p0 < /content/alphafold/docker/openmm.patch)\n",
        "\n",
        "# Put OpenMM and pdbfixer in ipython path which includes current directory /content\n",
        "ln -s /opt/conda/lib/python3.7/site-packages/simtk .\n",
        "ln -s /opt/conda/lib/python3.7/site-packages/pdbfixer .\n",
        "'''\n",
        "    run_shell_commands(cmds, 'install_openmm.sh', install_log)\n",
        "\n",
        "# ================================================================================================\n",
        "# Python routines to run a prediction.\n",
        "#\n",
        "\n",
        "# Check sequence\n",
        "def check_sequences(sequences, min_sequence_length = 16, max_sequence_length = 2500):\n",
        "    seqs = []\n",
        "    for seq in sequences:\n",
        "        # Remove all whitespaces, tabs and end lines; upper-case\n",
        "        seq = seq.translate(str.maketrans('', '', ' \\n\\t')).upper()\n",
        "        aatypes = set('ACDEFGHIKLMNPQRSTVWY')  # 20 standard aatypes\n",
        "        if not set(seq).issubset(aatypes):\n",
        "            raise Exception(f'Input sequence contains non-amino acid letters: {set(seq) - aatypes}. AlphaFold only supports 20 standard amino acids as inputs.')\n",
        "        if len(seq) < min_sequence_length:\n",
        "            raise Exception(f'Input sequence is too short: {len(seq)} amino acids, while the minimum is {min_sequence_length}')\n",
        "        if len(seq) > max_sequence_length:\n",
        "            raise Exception(f'Input sequence is too long: {len(seq)} amino acids, while the maximum is {max_sequence_length}. Please use the full AlphaFold system for long sequences.')\n",
        "        seqs.append(seq)\n",
        "    return seqs\n",
        "    \n",
        "def set_environment_variables():\n",
        "    # Set memory management environment variables used by TensorFlow and JAX\n",
        "    # These settings were suggested for longer sequences by SBGrid\n",
        "    #  https://sbgrid.org/wiki/examples/alphafold2\n",
        "    import os\n",
        "    os.environ['TF_FORCE_UNIFIED_MEMORY'] = '1'\n",
        "    os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.5'\n",
        "    os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "# Create directory for results and write target sequence file.\n",
        "def start_run(sequences, output_dir):\n",
        "    # Move previous results if sequence has changed.\n",
        "    from os import path, makedirs, rename\n",
        "    seq_files = [path.join(output_dir, 'sequence_%d.fasta' % (i+1)) for i in range(len(sequences))]\n",
        "    if path.exists(seq_files[0]):\n",
        "        if same_sequences(sequences, seq_files):\n",
        "            return seq_files\n",
        "        # Rename current results directory and zip file.\n",
        "        suffix = next_available_file_suffix(output_dir)\n",
        "        rename(output_dir, output_dir + suffix)\n",
        "        results_file = path.join(output_dir, '..', )\n",
        "        if path.exists('results.zip'):\n",
        "            rename('results.zip', 'results%s.zip' % suffix)\n",
        "\n",
        "    # Make new results directory\n",
        "    makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Write target sequence to file in FASTA format for doing search.\n",
        "    for seq, file in zip(sequences, seq_files):\n",
        "        with open(file, 'wt') as f:\n",
        "            f.write(f'>query\\n{seq}')\n",
        "\n",
        "    return seq_files\n",
        "\n",
        "def same_sequences(sequences, seq_files):\n",
        "    from os import path\n",
        "    for seq, file in zip(sequences, seq_files):\n",
        "        if not path.exists(file):\n",
        "            return False\n",
        "        last_seq = read_sequence(file)\n",
        "        if seq != last_seq:\n",
        "            return False\n",
        "    return True\n",
        "  \n",
        "def read_sequence(seq_file):\n",
        "    with open(seq_file, 'r') as f:\n",
        "        return ''.join(line.strip() for line in f.readlines()[1:])\n",
        "\n",
        "def next_available_file_suffix(path):\n",
        "    i = 1\n",
        "    import os.path\n",
        "    while os.path.exists(path + ('%d' % i)):\n",
        "        i += 1\n",
        "    return '%d' % i\n",
        "    \n",
        "# Make table of sequence databases to be searched\n",
        "def sequence_databases(multimer = False):\n",
        "    db_prefix = f'https://storage.googleapis.com/alphafold-colab%s/latest'\n",
        "    databases = [\n",
        "        {\n",
        "            'name': 'uniref90',\n",
        "            'url': db_prefix + '/uniref90_2021_03.fasta',\n",
        "            'num chunks':59,\n",
        "            'max hits': 10000,\n",
        "            'z value': 135301051\n",
        "        },\n",
        "        {\n",
        "            'name': 'smallbfd',\n",
        "            'url': db_prefix + '/bfd-first_non_consensus_sequences.fasta',\n",
        "            'num chunks': 17,\n",
        "            'max hits': 5000,\n",
        "            'z value': 65984053,\n",
        "        },\n",
        "        {\n",
        "            'name': 'mgnify',\n",
        "            'url': db_prefix + '/mgy_clusters_2019_05.fasta',\n",
        "            'num chunks': 71,\n",
        "            'max hits': 500,\n",
        "            'z value': 304820129,\n",
        "        },\n",
        "    ]\n",
        "    if multimer:\n",
        "        databases.append({\n",
        "            # Swiss-Prot and TrEMBL are concatenated together as UniProt.\n",
        "            'name': 'uniprot',\n",
        "            'url': db_prefix + '/uniprot_2021_03.fasta',\n",
        "            'num chunks': 98,\n",
        "            'max hits': 50000,\n",
        "            'z value': 219174961 + 565254})\n",
        "\n",
        "    if fast_test:\n",
        "        databases = [db for db in databases if db['name'] == 'smallbfd']\n",
        "        databases[0]['num chunks'] = 5\n",
        "    return databases\n",
        "\n",
        "# Find the fastest responding mirror for sequence databases\n",
        "def fastest_sequence_db_mirror(test_url_pattern = 'https://storage.googleapis.com/alphafold-colab{:s}/latest/uniref90_2021_03.fasta.1'):\n",
        "    print ('Finding fastest mirror for sequence databases', end = '')\n",
        "    from concurrent import futures\n",
        "    ex = futures.ThreadPoolExecutor(3)\n",
        "    def fetch(source):\n",
        "        from urllib import request\n",
        "        request.urlretrieve(test_url_pattern.format(source))\n",
        "        return source\n",
        "    fs = [ex.submit(fetch, source) for source in ['', '-europe', '-asia']]\n",
        "    source = None\n",
        "    for f in futures.as_completed(fs):\n",
        "      source = f.result()\n",
        "      ex.shutdown()\n",
        "      break\n",
        "    mirror = (source[1:] if source else 'united states')\n",
        "    print (' using', mirror)\n",
        "    return source\n",
        "\n",
        "# Search against 1 Gbyte chunks of a sequence database streamed from the web.\n",
        "def jackhmmer_sequence_search(seq_file, database, mirror = '',\n",
        "                              jackhmmer_binary_path = '/usr/bin/jackhmmer'):\n",
        "\n",
        "    db_name = database['name']\n",
        "    nchunks = database['num chunks']\n",
        "    db_url = database['url'] % mirror\n",
        "\n",
        "    print ('Searching %s sequence database, %d Gbytes' % (db_name, nchunks))\n",
        "    def progress_cb(i):\n",
        "        print (' %d' % i, end = ('\\n' if i%30 == 0 else ''), flush = True)\n",
        "\n",
        "    from alphafold.data.tools import jackhmmer\n",
        "    jackhmmer_runner = jackhmmer.Jackhmmer(\n",
        "        binary_path=jackhmmer_binary_path,\n",
        "        database_path=db_url,\n",
        "        get_tblout=True,\n",
        "        num_streamed_chunks=database['num chunks'],\n",
        "        streaming_callback = progress_cb,\n",
        "        z_value=database['z value'])\n",
        "\n",
        "    results = jackhmmer_runner.query(seq_file)\n",
        "\n",
        "    print ('')\n",
        "\n",
        "    return results\n",
        "\n",
        "def write_sequence_alignment(msa, database_name, target, output_dir):\n",
        "    prefix = '%s_%s' % (target, database_name)\n",
        "    from os import path\n",
        "    with open(path.join(output_dir, prefix + '_alignment'), 'w') as f:\n",
        "        for line in msa.sequences:\n",
        "            f.write(line + '\\n')\n",
        "    with open(path.join(output_dir, prefix + '_deletions'), 'w') as f:\n",
        "        for dcounts in msa.deletion_matrix:\n",
        "            f.write(','.join('%d' % count for count in dcounts) + '\\n')\n",
        "\n",
        "def read_sequence_alignment(database_name, target, output_dir):\n",
        "    from os import path\n",
        "    prefix = '%s_%s' % (target, database_name)\n",
        "    apath = path.join(output_dir, prefix + '_alignment')\n",
        "    dpath = path.join(output_dir, prefix + '_deletions')\n",
        "    if not path.exists(apath) or not path.exists(dpath):\n",
        "        return [],[]\n",
        "    with open(apath, 'r') as f:\n",
        "        seqs = [line.rstrip() for line in f.readlines()]\n",
        "    with open(dpath, 'r') as f:\n",
        "        dcounts = [[int(value) for value in line.split(',')] for line in f.readlines()]\n",
        "    return seqs, dcounts\n",
        "\n",
        "def search_sequence_databases(sequences, seq_files, databases, output_dir):\n",
        "    seq_msas = []\n",
        "    unique_seq_msas = {}\n",
        "    for seq_index, (seq, file) in enumerate(zip(sequences, seq_files)):\n",
        "        if seq in unique_seq_msas:\n",
        "            import copy\n",
        "            msas = copy.deepcopy(unique_seq_msas[seq])\n",
        "        else:\n",
        "            # Align\n",
        "            msas = create_multiple_sequence_alignments(file, databases, output_dir)\n",
        "            unique_seq_msas[seq] = msas\n",
        "            plot = plot_alignment_coverage(msas)\n",
        "            if plot:\n",
        "                import os.path\n",
        "                image_path = os.path.join(output_dir, 'sequence_coverage_%d.png' % (seq_index+1))\n",
        "                plot.savefig(image_path, bbox_inches='tight')\n",
        "        seq_msas.append(msas)\n",
        "    return seq_msas\n",
        "\n",
        "def create_multiple_sequence_alignments(sequence_file, databases, output_dir):\n",
        "    '''Search and make one multiple sequence alignment for each database.'''\n",
        "\n",
        "    from os.path import basename, splitext\n",
        "    target = splitext(basename(sequence_file))[0]\n",
        "\n",
        "    nchunks = sum(db['num chunks'] for db in databases)\n",
        "    print ('Searching sequence databases (%d Gbytes).' % nchunks)\n",
        "    print ('Search will take %d minutes or more.' % max(1,nchunks//5))\n",
        "\n",
        "    # Search for sequences\n",
        "    msas = []\n",
        "    mirror = None\n",
        "    from alphafold.notebooks import notebook_utils\n",
        "    for database in databases:\n",
        "        db_name = database['name']\n",
        "        alignment, deletions = read_sequence_alignment(db_name, target, output_dir)\n",
        "        if alignment:\n",
        "            from alphafold.data.parsers import Msa\n",
        "            descrips = ['%s %d' % (db_name,i+1) for i in range(len(alignment))]\n",
        "            msa = Msa(alignment, deletions, descrips)\n",
        "        else:\n",
        "            if mirror is None:\n",
        "                mirror = fastest_sequence_db_mirror()\n",
        "            db_results = jackhmmer_sequence_search(sequence_file, database, mirror)\n",
        "            # Make multiple sequence alignment.\n",
        "            print ('Merging chunk sequence alignments for %s' % db_name)\n",
        "            msa = notebook_utils.merge_chunked_msa(results=db_results, max_hits=database['max hits'])\n",
        "            write_sequence_alignment(msa, db_name, target, output_dir)\n",
        "        msas.append(msa)\n",
        "\n",
        "    return msas\n",
        "\n",
        "def plot_alignment_coverage(msas):\n",
        "    counts = alignment_coverage([msa.sequences for msa in msas])\n",
        "    if counts is None:\n",
        "        return\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure(figsize=(12, 3))\n",
        "    plt.title('Number of Aligned Sequences with no Gap for each Residue Position')\n",
        "    x = range(1, len(counts)+1)\t# Start residue numbers at 1, not 0.\n",
        "    plt.plot(x, counts, color='black')\n",
        "    plt.xlabel('Residue number')\n",
        "    plt.ylabel('Coverage')\n",
        "    plot = plt.gcf()\n",
        "    plt.show()\n",
        "    return plot\n",
        "\n",
        "def alignment_coverage(alignments):\n",
        "    counts = None\n",
        "    for alignment in alignments:\n",
        "        for line in alignment:\n",
        "            if counts is None:\n",
        "                from numpy import zeros, int32\n",
        "                counts = zeros((len(line),), int32)\n",
        "            for i,c in enumerate(line):\n",
        "                if c != '-':\n",
        "                    counts[i] += 1\n",
        "    return counts\n",
        "\n",
        "# Create input for AlphaFold from sequence alignments.\n",
        "def features(sequences, seq_msas):\n",
        "    features_for_chain = {}\n",
        "    heteromer = len(set(sequences)) > 1\n",
        "    for seq_index, (seq, msas) in enumerate(zip(sequences, seq_msas)):\n",
        "            \n",
        "        # Create features dictionary\n",
        "        feature_dict = {}\n",
        "        from alphafold.data import pipeline\n",
        "        feature_dict.update(pipeline.make_sequence_features(sequence=seq, description='query',\n",
        "                                                            num_res=len(seq)))\n",
        "        feature_dict.update(pipeline.make_msa_features(msas=msas))\n",
        "        from alphafold.notebooks import notebook_utils\n",
        "        placeholder_features = notebook_utils.empty_placeholder_template_features(num_templates=0,\n",
        "                                                                                  num_res=len(seq))\n",
        "        feature_dict.update(placeholder_features)\n",
        "\n",
        "        # Construct the all_seq features only for heteromers, not homomers.\n",
        "        if heteromer:\n",
        "            from alphafold.data import msa_pairing\n",
        "            valid_feats = msa_pairing.MSA_FEATURES + (\n",
        "                'msa_uniprot_accession_identifiers',\n",
        "                'msa_species_identifiers',\n",
        "            )\n",
        "            uniprot_msa = msas[-1]  # Last alignment is for uniprot database\n",
        "            all_seq_features = {\n",
        "                f'{k}_all_seq': v for k, v in pipeline.make_msa_features([uniprot_msa]).items()\n",
        "                if k in valid_feats}\n",
        "            feature_dict.update(all_seq_features)\n",
        "\n",
        "        from alphafold.common import protein\n",
        "        chain_id = protein.PDB_CHAIN_IDS[seq_index]\n",
        "        features_for_chain[chain_id] = feature_dict\n",
        "\n",
        "    # Do further feature post-processing depending on the model type.\n",
        "    multimer = (len(sequences) > 1)\n",
        "    if multimer:\n",
        "        all_chain_features = {}\n",
        "        from alphafold.data import pipeline_multimer\n",
        "        for chain_id, chain_features in features_for_chain.items():\n",
        "            all_chain_features[chain_id] = pipeline_multimer.convert_monomer_features(\n",
        "                chain_features, chain_id)\n",
        "        all_chain_features = pipeline_multimer.add_assembly_features(all_chain_features)\n",
        "        from alphafold.data import feature_processing\n",
        "        feature_dict = feature_processing.pair_and_merge(all_chain_features=all_chain_features)\n",
        "    else:\n",
        "        from alphafold.common import protein\n",
        "        feature_dict = features_for_chain[protein.PDB_CHAIN_IDS[0]]\n",
        "\n",
        "    return feature_dict\n",
        "\n",
        "# Predict the structures\n",
        "def predict_structure(feature_dict, multimer, model_name, output_dir):\n",
        "\n",
        "    from alphafold.model import config, data, model\n",
        "    cfg = config.model_config(model_name)\n",
        "    if multimer:\n",
        "        cfg.model.num_ensemble_eval = 1\n",
        "    else:\n",
        "        cfg.data.eval.num_ensemble = 1\n",
        "    params = data.get_model_haiku_params(model_name, './alphafold/data')\n",
        "    model_runner = model.RunModel(cfg, params)\n",
        "    processed_feature_dict = model_runner.process_features(feature_dict, random_seed=0)\n",
        "    prediction_result = model_runner.predict(processed_feature_dict, random_seed=0)\n",
        "\n",
        "    if 'predicted_aligned_error' in prediction_result:\n",
        "        pae_output = (\n",
        "            prediction_result['predicted_aligned_error'],\n",
        "            prediction_result['max_predicted_aligned_error']\n",
        "        )\n",
        "    else:\n",
        "        pae_output = None\n",
        "    plddt = prediction_result['plddt']\n",
        "\n",
        "    # Set the b-factors to the per-residue plddt.\n",
        "    final_atom_mask = prediction_result['structure_module']['final_atom_mask']\n",
        "    b_factors = prediction_result['plddt'][:, None] * final_atom_mask\n",
        "    from alphafold.common import protein\n",
        "    unrelaxed_protein = protein.from_prediction(processed_feature_dict,\n",
        "                                                prediction_result,\n",
        "                                                b_factors=b_factors,\n",
        "                                                remove_leading_feature_dimension = not multimer)\n",
        "\n",
        "    # Delete unused outputs to save memory.\n",
        "    del model_runner\n",
        "    del params\n",
        "    del prediction_result\n",
        "\n",
        "    score = plddt.mean()\n",
        "    write_unrelaxed_pdb(model_name, unrelaxed_protein, score, pae_output, output_dir)\n",
        "    \n",
        "    return unrelaxed_protein, plddt, pae_output\n",
        "\n",
        "def write_unrelaxed_pdb(model_name, unrelaxed_protein, score, pae_output, output_dir):\n",
        "    # Write out PDB files and predicted alignment error\n",
        "    from alphafold.common import protein\n",
        "    write_pdb(protein.to_pdb(unrelaxed_protein), model_name + '_unrelaxed.pdb', output_dir)\n",
        "\n",
        "    from os import path\n",
        "    with open(path.join(output_dir, model_name + '_score'), 'w') as f:\n",
        "        f.write('%.5g' % score)\n",
        "\n",
        "    # Save predicted aligned error (if it exists)\n",
        "    if pae_output is not None:\n",
        "        pae_output_path = path.join(output_dir, model_name + '_pae.json')\n",
        "        save_predicted_aligned_error(pae_output, pae_output_path)\n",
        "\n",
        "def best_model(model_names, output_dir):\n",
        "    best_score = None\n",
        "    from os import path\n",
        "    for name in model_names:\n",
        "        spath = path.join(output_dir, name + '_score')\n",
        "        if path.exists(spath):\n",
        "            with open(spath, 'r') as f:\n",
        "                score = float(f.readline())\n",
        "            if best_score is None or score > best_score:\n",
        "                best_score, best_model_name = score, name\n",
        "\n",
        "    if best_score is None:\n",
        "        print('No models successfully computed.')\n",
        "        return None\n",
        "\n",
        "    return best_model_name\n",
        "\n",
        "def minimize_best_model(best_model_name, output_dir):\n",
        "    # Energy minimize the best model\n",
        "    from os import path\n",
        "    relaxed_path = path.join(output_dir, best_model_name + '_relaxed.pdb')\n",
        "    if path.exists(relaxed_path):\n",
        "        return\n",
        "    \n",
        "    print('Energy minimizing best structure %s with OpenMM and Amber forcefield' % best_model_name)\n",
        "    unrelaxed_path = path.join(output_dir, best_model_name + '_unrelaxed.pdb')\n",
        "    from alphafold.common import protein\n",
        "    with open(unrelaxed_path, 'r') as f:\n",
        "        best_unrelaxed_protein = protein.from_pdb_string(f.read())\n",
        "        relaxed_pdb = energy_minimize_structure(best_unrelaxed_protein)\n",
        "        # Write out PDB file\n",
        "        write_pdb(relaxed_pdb, best_model_name + '_relaxed.pdb', output_dir)\n",
        "\n",
        "def energy_minimize_structure(pdb_model):\n",
        "    from alphafold.relax import relax\n",
        "    amber_relaxer = relax.AmberRelaxation(\n",
        "        max_iterations=0,\n",
        "        tolerance=2.39,\n",
        "        stiffness=10.0,\n",
        "        exclude_residues=[],\n",
        "        max_outer_iterations=1,\n",
        "        use_gpu=True)\n",
        "    relaxed_pdb, _, _ = amber_relaxer.process(prot=pdb_model)\n",
        "    return relaxed_pdb\n",
        "\n",
        "def write_pdb(pdb_model, filename, output_dir):\n",
        "    import os.path\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(pdb_model)\n",
        "\n",
        "def copy_file(filename, new_filename, output_dir):\n",
        "    # Copy PAE file if it exists.\n",
        "    from os import path\n",
        "    from_path = path.join(output_dir, filename)\n",
        "    if path.exists(from_path):\n",
        "        to_path = path.join(output_dir, new_filename)\n",
        "        import os\n",
        "        if path.exists(to_path):\n",
        "            os.remove(to_path)\n",
        "        os.link(from_path, to_path)\n",
        "\n",
        "def save_predicted_aligned_error(model_pae, pae_output_path):\n",
        "  # Save predicted aligned error in the same format as the AF EMBL DB\n",
        "  pae, max_pae = model_pae\n",
        "  import numpy as np\n",
        "  rounded_errors = np.round(pae.astype(np.float64), decimals=1)\n",
        "  indices = np.indices((len(rounded_errors), len(rounded_errors))) + 1\n",
        "  indices_1 = indices[0].flatten().tolist()\n",
        "  indices_2 = indices[1].flatten().tolist()\n",
        "  pae_data = [{\n",
        "      'residue1': indices_1,\n",
        "      'residue2': indices_2,\n",
        "      'distance': rounded_errors.flatten().tolist(),\n",
        "      'max_predicted_aligned_error': max_pae.item()\n",
        "  }]\n",
        "  import json\n",
        "  json_data = json.dumps(pae_data, indent=None, separators=(',', ':'))\n",
        "  with open(pae_output_path, 'w') as f:\n",
        "    f.write(json_data)\n",
        "\n",
        "def run_prediction(sequences,\n",
        "                   model_names = None,\n",
        "                   energy_minimize = True,\n",
        "                   output_dir = 'prediction',\n",
        "                   install_log = 'install_log.txt'):\n",
        "    '''\n",
        "    Installs alphafold if not yet installed and runs a stucture prediction.\n",
        "    Model names ending in \"_ptm\" predict TM score ('model_1_ptm', ..., 'model_5_ptm').\n",
        "    '''\n",
        "\n",
        "    print('Using AlphaFold 2.2.0')\n",
        "    \n",
        "    # Check sequence length are within limits and no illegal characters\n",
        "    sequences = check_sequences(sequences)\n",
        "    if len(sequences) == 1:\n",
        "        msg = 'Sequence length %d' % len(sequences[0])\n",
        "    else:\n",
        "        msg = ('%d sequences, total length %d (= %s)' %\n",
        "               (len(sequences), sum(len(seq) for seq in sequences),\n",
        "                ' + '.join('%d' % len(seq) for seq in sequences)))\n",
        "    print(msg)\n",
        "\n",
        "    # Check for GPU at beginning.\n",
        "    # If no GPU then enabling a GPU runtime clears all virtual machine state\n",
        "    # so need to enable GPU runtime before installing the prerequisites.\n",
        "    check_for_gpu()\n",
        "\n",
        "    # Install sequence search software, alphafold and OpenMM for energy minimization.\n",
        "    if not is_alphafold_installed():\n",
        "        print ('Installing HMMER for computing sequence alignments')\n",
        "        install_hmmer(install_log = install_log)\n",
        "        print ('Installing matplotlib to plot sequence alignment coverage')\n",
        "        install_matplotlib(install_log = install_log)\n",
        "        print ('Installing AlphaFold')\n",
        "        install_alphafold(install_log = install_log)\n",
        "    if energy_minimize and not is_openmm_installed():\n",
        "        print ('Installing OpenMM for structure energy minimization')\n",
        "        install_openmm(install_log = install_log)\n",
        "\n",
        "    set_environment_variables()\n",
        "\n",
        "    # Create directory for results and write sequence file.\n",
        "    seq_files = start_run(sequences, output_dir)\n",
        "\n",
        "    # Search sequence databases producing a multiple sequence alignments\n",
        "    # for each sequence against each database.\n",
        "    multimer = (len(sequences) > 1)\n",
        "    databases = sequence_databases(multimer)\n",
        "    seq_msas = search_sequence_databases(sequences, seq_files, databases, output_dir)\n",
        "\n",
        "    # Create features dictionary input to AlphaFold\n",
        "    feature_dict = features(sequences, seq_msas)\n",
        "\n",
        "    # Choose which AlphaFold neural networks to use.\n",
        "    if model_names is None:\n",
        "        from alphafold.model import config\n",
        "        model_names = config.MODEL_PRESETS['multimer' if multimer else 'monomer_ptm']\n",
        "    if fast_test:\n",
        "        model_names = model_names[:1]\n",
        "\n",
        "    # Predict structures by running AlphaFold\n",
        "    print('Computing structures using %d AlphaFold parameter sets:' % len(model_names))\n",
        "    from os import path\n",
        "    for model_name in model_names:\n",
        "        if not path.exists(path.join(output_dir, model_name + '_unrelaxed.pdb')):\n",
        "            print(' ' + model_name, end = '', flush = True)\n",
        "            try:\n",
        "                predict_structure(feature_dict, multimer, model_name, output_dir)\n",
        "            except Exception:\n",
        "                error_log_path = path.join(output_dir, model_name + '_error')\n",
        "                import traceback\n",
        "                with open(error_log_path, 'w') as f:\n",
        "                    traceback.print_exc(file = f)\n",
        "                print ('\\nAlphaFold generated an error computing %s, error logged to %s\\n'\n",
        "                       % (model_name, error_log_path))\n",
        "    print('')\n",
        "\n",
        "    # Energy minimize\n",
        "    best_model_name = best_model(model_names, output_dir)\n",
        "    if best_model_name:\n",
        "        if energy_minimize:\n",
        "            minimize_best_model(best_model_name, output_dir)\n",
        "        # Copy best model and pae files.\n",
        "        pdb_suffix = '_relaxed.pdb' if energy_minimize else '_unrelaxed.pdb'\n",
        "        copy_file(best_model_name + pdb_suffix, 'best_model.pdb', output_dir)\n",
        "        copy_file(best_model_name + '_pae.json', 'best_model_pae.json', output_dir)\n",
        "        \n",
        "    print ('Structure prediction completed.')\n",
        "\n",
        "    # Make a zip file of the predictions\n",
        "    !cd {output_dir} ; zip -q -r ../results.zip *\n",
        "    \n",
        "    # Download predictions.\n",
        "    from google.colab import files\n",
        "    files.download('results.zip')\n",
        "\n",
        "# ================================================================================================\n",
        "# Predict a structure for a sequence.\n",
        "#\n",
        "fast_test = False\n",
        "sequences = 'Paste a sequences separated by commas here'  #@param {type:\"string\"}\n",
        "\n",
        "seq_list = sequences.split(',')\n",
        "dont_minimize = (seq_list[0] == 'dont_minimize')\n",
        "if dont_minimize:\n",
        "    seq_list = seq_list[1:]\n",
        "\n",
        "# Remove obsolete \"prokaryote\" flag\n",
        "is_prokaryote = (seq_list[0] == 'prokaryote')\n",
        "if is_prokaryote:\n",
        "    seq_list = seq_list[1:]\n",
        "\n",
        "run_prediction(seq_list, energy_minimize = not dont_minimize)\n"
      ]
    }
  ]
}