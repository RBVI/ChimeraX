Common subdirectories: tcia_utils.orig/__pycache__ and tcia_utils/__pycache__
diff --color -u tcia_utils.orig/datacite.py tcia_utils/datacite.py
--- tcia_utils.orig/datacite.py	2023-07-10 14:54:52
+++ tcia_utils/datacite.py	2023-07-10 14:55:39
@@ -1,13 +1,12 @@
-import pandas as pd
 import requests
 from datetime import datetime
 import logging

 _log = logging.getLogger(__name__)
-logging.basicConfig(
-    format='%(asctime)s:%(levelname)s:%(message)s'
-    , level=logging.INFO
-)
+#logging.basicConfig(
+#    format='%(asctime)s:%(levelname)s:%(message)s'
+#    , level=logging.INFO
+#)

 ####### getDoi function
 # Gets metadata for one or more DOIs
@@ -24,6 +23,7 @@
            license = "",
            format = ""):

+    import pandas as pd
     datacite_url = "https://api.datacite.org/dois/"
     datacite_headers = {"accept": "application/vnd.api+json"}
     df = pd.DataFrame()
@@ -106,25 +106,25 @@
                     citation_count = item["attributes"]["citationCount"]
                     reference_count = item["attributes"]["referenceCount"]
                     related = f"{relation_type}: {related_identifier}" if relation_type and related_identifier else None
-                    dois.append({"DOI": doi,
-                                "Identifier": identifier,
+                    dois.append({"DOI": doi,
+                                "Identifier": identifier,
                                 "CreatorNames": ", ".join(creator_names),
-                                "Title": title,
-                                "Created": created,
-                                "Updated": updated,
-                                "Related": related,
-                                "Version": version,
-                                "Rights": rights,
-                                "RightsURI": rights_uri,
-                                "Description": description,
-                                "FundingReferences": funding_references,
-                                "URL": url,
-                                "CitationCount": citation_count,
+                                "Title": title,
+                                "Created": created,
+                                "Updated": updated,
+                                "Related": related,
+                                "Version": version,
+                                "Rights": rights,
+                                "RightsURI": rights_uri,
+                                "Description": description,
+                                "FundingReferences": funding_references,
+                                "URL": url,
+                                "CitationCount": citation_count,
                                 "ReferenceCount": reference_count})

-                df = pd.DataFrame(dois, columns=["DOI", "Identifier", "CreatorNames", "Title", "Created", "Updated", "Related",
-                                                  "Version", "Rights", "RightsURI", "Description", "FundingReferences", "URL",
-                                                  "CitationCount", "ReferenceCount"])
+                df = pd.DataFrame(dois, columns=["DOI", "Identifier", "CreatorNames", "Title", "Created", "Updated", "Related",
+                                                  "Version", "Rights", "RightsURI", "Description", "FundingReferences", "URL",
+                                                  "CitationCount", "ReferenceCount"])
                 if format == "csv":
                     now = datetime.now()
                     dt_string = now.strftime("%Y-%m-%d_%H%M")
@@ -135,7 +135,7 @@
                 return data
         else:
             _log.info(f'No results found.')
-
+
     # handle errors
     except requests.exceptions.HTTPError as errh:
         _log.error(f'Error: {errh}')
@@ -144,4 +144,4 @@
     except requests.exceptions.Timeout as errt:
         _log.error(f'Error: {errt}')
     except requests.exceptions.RequestException as err:
-        _log.error(f'Error: {err}')
\ No newline at end of file
+        _log.error(f'Error: {err}')
diff --color -u tcia_utils.orig/nbia.py tcia_utils/nbia.py
--- tcia_utils.orig/nbia.py	2023-07-10 14:54:52
+++ tcia_utils/nbia.py	2023-07-10 14:55:34
@@ -1,7 +1,6 @@
 ####### setup
 import logging
 import requests
-import pandas as pd
 import getpass
 import json
 import zipfile
@@ -13,17 +12,15 @@
-import matplotlib.pyplot as plt
 import pydicom
 import numpy as np
-from ipywidgets import interact

 class StopExecution(Exception):
     def _render_traceback_(self):
         pass

 _log = logging.getLogger(__name__)
-logging.basicConfig(
-    format='%(asctime)s:%(levelname)s:%(message)s'
-    , level=logging.INFO
-)
+#logging.basicConfig(
+#    format='%(asctime)s:%(levelname)s:%(message)s'
+#    , level=logging.INFO
+#)

 # Used by functions that accept parameters used in GUI Simple Search
 # e.g. getSimpleSearchWithModalityAndBodyPartPaged()
@@ -50,14 +48,14 @@
 def setApiUrl(endpoint, api_url):
     """
     setApiUrl() is used by most other functions to select the correct base URL
-    and is generally not something that needs to be called directly in your code.
-
+    and is generally not something that needs to be called directly in your code.
+
     It assists with:
         1. verifying you are calling a supported endpoint
         2. selecting the correct base URL for Search vs Advanced APIs
         3. selecting the correct base URL for regular collections vs NLST
         4. ensuring you have a valid security token where necessary
-
+
     Learn more about the NBIA APIs at https://wiki.cancerimagingarchive.net/x/ZoATBg
     """
     global searchEndpoints, advancedEndpoints
@@ -108,7 +106,7 @@
                     getToken(user = "nbia_guest", api_url = "nlst")
                 if 'nlst_token_exp_time' in globals() and datetime.now() > nlst_token_exp_time:
                     refreshToken(api_url = "nlst")
-                base_url = "https://nlst.cancerimagingarchive.net/nbia-api/services/"
+                base_url = "https://nlst.cancerimagingarchive.net/nbia-api/services/"
         elif api_url == "restricted":
             if endpoint in searchEndpoints:
                 # Using "Search with Authentication" API (login required): https://wiki.cancerimagingarchive.net/x/X4ATBg
@@ -146,7 +144,7 @@

         return base_url

-def getToken(user = "", pw = "", api_url = ""):
+def getToken(user = "", pw = "", api_url = ""):
     """
     getToken() accepts user and pw parameters to create a token to access APIs that require authorization.
     Access tokens expire 2 hours after creation, and can be refreshed with refreshToken().
@@ -160,7 +158,7 @@
     if api_url == "nlst":
         token_url = "https://nlst.cancerimagingarchive.net/nbia-api/oauth/token"
         userName = "nbia_guest"
-        passWord = ""
+        passWord = ""
     # specify user/pw unless nbia_guest is being used for accessing Advanced API anonymously
     else:
         token_url = "https://nbia.cancerimagingarchive.net/nbia-api/oauth/token"
@@ -185,7 +183,7 @@
         'username' : userName,
         'password': passWord
         }
-
+
         data = requests.post(token_url, data = params)
         data.raise_for_status()
         access_token = data.json()["access_token"]
@@ -198,7 +196,7 @@
             nlst_refresh_token = data.json()["refresh_token"]
             _log.info(f'Success - Token saved to nlst_api_call_headers variable and expires at {nlst_token_exp_time}')
         else:
-            token_exp_time = current_time + timedelta(seconds=expires_in)
+            token_exp_time = current_time + timedelta(seconds=expires_in)
             api_call_headers = {'Authorization': 'Bearer ' + access_token}
             refresh_token = data.json()["refresh_token"]
             _log.info(f'Success - Token saved to api_call_headers variable and expires at {token_exp_time}')
@@ -216,10 +214,10 @@
     """
     refreshToken() refreshes security tokens to extend access time for APIs that require authorization.
     It attempts to verify that a refresh token exists and recommends using getToken() to create a new token if needed.
-    This function is called as needed by setApiUrl() and is generally not something that needs to be called directly in your code.
+    This function is called as needed by setApiUrl() and is generally not something that needs to be called directly in your code.
     """
     global token_exp_time, nlst_token_exp_time, api_call_headers, nlst_api_call_headers
-
+
     # token URLs
     if api_url == "nlst":
         try:
@@ -230,7 +228,7 @@
             raise StopExecution
         else:
             token_url = "https://nlst.cancerimagingarchive.net/nbia-api/oauth/token"
-
+
     else:
         try:
             token = refresh_token
@@ -248,7 +246,7 @@
         'grant_type': 'refresh_token',
         'refresh_token' : token,
         }
-
+
         # obtain new access token
         data = requests.post(token_url, data = params)
         data.raise_for_status()
@@ -273,8 +271,8 @@
         _log.error(f"Timeout Error: {data.status_code}")
     except requests.exceptions.RequestException as err:
         _log.error(f"Request Error: {data.status_code}")
-
-def logoutToken(api_url = ""):
+
+def logoutToken(api_url = ""):
     """
     logoutToken() logs out security tokens used for APIs that require authorization.
     Variables holding access token information are also deleted by this operation.
@@ -289,7 +287,7 @@
         api_call_headers = nlst_api_call_headers
     elif api_url != "nlst" and 'api_call_headers' in globals():
         url = "https://services.cancerimagingarchive.net/nbia-api/logout"
-    else:
+    else:
         _log.error("Error: You haven't created a token yet, or have already logged out.")
         raise StopExecution

@@ -315,14 +313,14 @@

 def makeCredentialFile(user = "", pw = ""):
     """
-    Creates a credential file to use with NBIA Data Retriever.
+    Creates a credential file to use with NBIA Data Retriever.
     Interactive prompts are provided for user/pw if they're not specified as parameters.
     The credential file is a text file that passes the user's credentials in the following format:
         userName = YourUserName
         passWord = YourPassword
         Both parameters are case-sensitive.
     Additional documentation:
-        https://wiki.cancerimagingarchive.net/x/2QKPBQ
+        https://wiki.cancerimagingarchive.net/x/2QKPBQ
         https://github.com/kirbyju/TCIA_Notebooks/blob/main/TCIA_Linux_Data_Retriever_App.ipynb
     """
     # set user name and password
@@ -396,8 +394,8 @@
         _log.error(errt)
     except requests.exceptions.RequestException as err:
         _log.error(err)
-

+
 def getCollections(api_url = "",
                    format = ""):
     """
@@ -822,7 +820,7 @@

     # get base URL
     base_url = setApiUrl(endpoint, api_url)
-
+
     # if input = manifest convert manifest to python list of uids
     if input_type == "manifest":
         series_data = manifestToList(series_data)
@@ -1182,7 +1180,7 @@

     data = queryData(endpoint, options, api_url, format)
     return data
-
+
 ####### getSegRefSeries function (Advanced)
 # Gets DICOM tag metadata for a given SEG/RTSTRUCT series UID (scan)
 # and then look up the series UID they were derived from
@@ -1208,7 +1206,7 @@
                 # Retrieve the value of "Series Instance UID" from the next row
                 refSeriesUid = df.loc[index + 1, 'data']
                 return refSeriesUid
-
+
             else:
                 print("Segmentation doesn't contain a reference series UID.")
                 refSeriesUid = "N/A"
@@ -1226,7 +1224,7 @@
                 # Retrieve the value of "Series Instance UID" from the next row
                 refSeriesUid = df.loc[index + 1, 'data']
                 return refSeriesUid
-
+
             else:
                 print("Segmentation doesn't contain a reference series UID.")
                 refSeriesUid = "N/A"
@@ -1471,7 +1469,7 @@
 # Specify input_type = "manifest" to ingest a *.TCIA manifest file or "list" for a python list of UIDs
 # If input_type = "manifest" or "list" and there are series UIDs that are restricted
 #    you must call getToken() with a user ID that has access to all UIDs before calling this function.
-# Specifying api_url is only necessary if you are using input_type = "manifest" or "list" with NLST data (e.g. api_url = "nlst")
+# Specifying api_url is only necessary if you are using input_type = "manifest" or "list" with NLST data (e.g. api_url = "nlst")
 # Specify format = "var" to return the report values as a dictionary
 # Access variables example after saving function output to report_data: subjects = report_data["subjects"]
 # Specify format = "file" to save the report to a file
@@ -1490,7 +1488,7 @@
     # if input_type is manifest convert it to a list
     if input_type == "manifest":
         series_data = manifestToList(series_data)
-
+
     # if input_type is a list or manifest download relevant metadata
     if input_type == "list" or input_type == "manifest":
         df = getSeriesList(series_data, api_url = "", csv_filename = "")
@@ -1663,6 +1661,7 @@
     Leave seriesUid empty if you want to provide a custom path.
     The function assumes "tciaDownload/<seriesUid>/" as path if seriesUid is provided since this is where downloadSeries() saves data.
     """
+    from ipywidgets import interact
     # set path where downloadSeries() saves the data if seriesUid is provided
     if seriesUid != "":
         path = "tciaDownload/" + seriesUid
diff --color -u tcia_utils.orig/pathdb.py tcia_utils/pathdb.py
--- tcia_utils.orig/pathdb.py	2023-07-10 14:54:52
+++ tcia_utils/pathdb.py	2023-07-10 14:55:45
@@ -1,13 +1,12 @@
-import pandas as pd
 import requests
 from datetime import datetime
 import logging

 _log = logging.getLogger(__name__)
-logging.basicConfig(
-    format='%(asctime)s:%(levelname)s:%(message)s'
-    , level=logging.INFO
-)
+#logging.basicConfig(
+#    format='%(asctime)s:%(levelname)s:%(message)s'
+#    , level=logging.INFO
+#)

 base_url = 'https://pathdb.cancerimagingarchive.net/'

@@ -19,6 +18,7 @@

 def getCollections(query = "", format = ""):

+    import pandas as pd
     extracted_data = []
     url = base_url + 'collections?_format=json'
     _log.info(f'Calling... {url}')
@@ -57,7 +57,7 @@
         _log.info(f"File saved to {filename}.")
     else:
         return extracted_data
-
+
 ###################
 # getImages()
 # use "query" parameter to search collection names or enter a specific collection ID
@@ -65,12 +65,13 @@
 #    or "csv" to save it to a file

 def getImages(query, format=""):
-
+
     collectionList = []  # for queries that match multiple collection names
+    import pandas as pd
     extracted_data = []

     def getPaginatedResults(id):
-        page = 0
+        page = 0

         while True:
             url = base_url + 'listofimages/' + str(id) + '?page=' + str(page) + '&_format=json'
@@ -80,7 +81,7 @@
                 data = response.json()
                 if len(data) == 0:
                     break  # No more pages, exit the loop
-
+
                 # Extract desired fields from the JSON data
                 for item in data:
                     extracted_item = {}
@@ -99,12 +100,12 @@
                 return None
             page += 1
         return extracted_data
-
+
     # if query was a collection ID (integer)
     if isinstance(query,int):
         extracted_data = getPaginatedResults(query)
     # if query is a string, look for matching collection names
-    else:
+    else:
         collections = getCollections(query=query)
         # iterate through all collections that matched query
         for x in collections:
@@ -121,4 +122,4 @@
         filename = f"pathologyImages-{today}.csv"
         df.to_csv(filename, index=False)
     else:
-        return extracted_data
\ No newline at end of file
+        return extracted_data
